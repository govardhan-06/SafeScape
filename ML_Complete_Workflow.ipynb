{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/govardhan-06/SafeScape/blob/mlBackend/ML_Complete_Workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZrpEDmo3Ffs"
      },
      "outputs": [],
      "source": [
        "content='''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "__all__ = ['inception_iccv']\n",
        "\n",
        "def inception_iccv(pretrained=True, debug=False, **kwargs):\n",
        "    model = InceptionNet(**kwargs)\n",
        "    \"\"\"\n",
        "        Pretrained model: 'https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/bninception.py'\n",
        "        Initializing with basedline models (trained BN-Inception) can obtain better results.\n",
        "    \"\"\"\n",
        "    if pretrained:\n",
        "        pretrained_dict = torch.load('model/bn_inception-52deb4733.pth')\n",
        "        model_dict = model.state_dict()\n",
        "        new_dict = {}\n",
        "        for k,_ in model_dict.items():\n",
        "            raw_name = k.replace('main_branch.', '')\n",
        "            if raw_name in pretrained_dict:\n",
        "                new_dict[k] = pretrained_dict[raw_name]\n",
        "        model_dict.update(new_dict)\n",
        "        model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "class ChannelAttn(nn.Module):\n",
        "    def __init__(self, in_channels, reduction_rate=16):\n",
        "        super(ChannelAttn, self).__init__()\n",
        "        assert in_channels%reduction_rate == 0\n",
        "        self.conv1 = nn.Conv2d(in_channels, in_channels // reduction_rate, kernel_size=1, stride=1, padding=0)\n",
        "        self.conv2 = nn.Conv2d(in_channels // reduction_rate, in_channels, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # squeeze operation (global average pooling)\n",
        "        x = F.avg_pool2d(x, x.size()[2:])\n",
        "        # excitation operation (2 conv layers)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.conv2(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class SpatialTransformBlock(nn.Module):\n",
        "    def __init__(self, num_classes, pooling_size, channels):\n",
        "        super(SpatialTransformBlock, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.spatial = pooling_size\n",
        "\n",
        "        self.global_pool = nn.AvgPool2d((pooling_size, pooling_size//2), stride=1, padding=0, ceil_mode=True, count_include_pad=True)\n",
        "\n",
        "        self.gap_list = nn.ModuleList()\n",
        "        self.fc_list = nn.ModuleList()\n",
        "        self.att_list = nn.ModuleList()\n",
        "        self.stn_list = nn.ModuleList()\n",
        "        for i in range(self.num_classes):\n",
        "            self.gap_list.append(nn.AvgPool2d((pooling_size, pooling_size//2), stride=1, padding=0, ceil_mode=True, count_include_pad=True))\n",
        "            self.fc_list.append(nn.Linear(channels, 1))\n",
        "            self.att_list.append(ChannelAttn(channels))\n",
        "            self.stn_list.append(nn.Linear(channels, 4))\n",
        "\n",
        "    def stn(self, x, theta):\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        x = F.grid_sample(x, grid, padding_mode='border')\n",
        "        return x.cuda()\n",
        "\n",
        "    def transform_theta(self, theta_i, region_idx):\n",
        "        theta = torch.zeros(theta_i.size(0), 2, 3)\n",
        "        theta[:,0,0] = torch.sigmoid(theta_i[:,0])\n",
        "        theta[:,1,1] = torch.sigmoid(theta_i[:,1])\n",
        "        theta[:,0,2] = torch.tanh(theta_i[:,2])\n",
        "        theta[:,1,2] = torch.tanh(theta_i[:,3])\n",
        "        theta = theta.cuda()\n",
        "        return theta\n",
        "\n",
        "    def forward(self, features):\n",
        "        pred_list = []\n",
        "        bs = features.size(0)\n",
        "        for i in range(self.num_classes):\n",
        "            stn_feature = features * self.att_list[i](features) + features\n",
        "\n",
        "            theta_i = self.stn_list[i](F.avg_pool2d(stn_feature, stn_feature.size()[2:]).view(bs,-1)).view(-1,4)\n",
        "            theta_i = self.transform_theta(theta_i, i)\n",
        "\n",
        "            sub_feature = self.stn(stn_feature, theta_i)\n",
        "            pred = self.gap_list[i](sub_feature).view(bs,-1)\n",
        "            pred = self.fc_list[i](pred)\n",
        "            pred_list.append(pred)\n",
        "        pred = torch.cat(pred_list, 1)\n",
        "        return pred\n",
        "\n",
        "\n",
        "class InceptionNet(nn.Module):\n",
        "    def __init__(self, num_classes=51):\n",
        "        super(InceptionNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.main_branch = BNInception()\n",
        "        self.global_pool = nn.AvgPool2d((8,4), stride=1, padding=0, ceil_mode=True, count_include_pad=True)\n",
        "        self.finalfc = nn.Linear(1024, num_classes)\n",
        "\n",
        "        self.st_3b = SpatialTransformBlock(num_classes, 32, 256*3)\n",
        "        self.st_4d = SpatialTransformBlock(num_classes, 16, 256*2)\n",
        "        self.st_5b = SpatialTransformBlock(num_classes, 8, 256)\n",
        "\n",
        "        # Lateral layers\n",
        "        self.latlayer_3b = nn.Conv2d(320, 256, kernel_size=1, stride=1, padding=0)\n",
        "        self.latlayer_4d = nn.Conv2d(608, 256, kernel_size=1, stride=1, padding=0)\n",
        "        self.latlayer_5b = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def _upsample_add(self, x, y):\n",
        "        _,_,H,W = y.size()\n",
        "        up_feat = F.interpolate(x, (H, W), mode='bilinear', align_corners=False)\n",
        "        return torch.cat([up_feat,y], 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        bs = input.size(0)\n",
        "        feat_3b, feat_4d, feat_5b = self.main_branch(input)\n",
        "        main_feat = self.global_pool(feat_5b).view(bs,-1)\n",
        "        main_pred = self.finalfc(main_feat)\n",
        "\n",
        "        fusion_5b = self.latlayer_5b(feat_5b)\n",
        "        fusion_4d = self._upsample_add(fusion_5b, self.latlayer_4d(feat_4d))\n",
        "        fusion_3b = self._upsample_add(fusion_4d, self.latlayer_3b(feat_3b))\n",
        "\n",
        "        pred_3b = self.st_3b(fusion_3b)\n",
        "        pred_4d = self.st_4d(fusion_4d)\n",
        "        pred_5b = self.st_5b(fusion_5b)\n",
        "\n",
        "        return pred_3b, pred_4d, pred_5b, main_pred\n",
        "\n",
        "class BNInception(nn.Module):\n",
        "    \"\"\"\n",
        "        Copy from 'https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/bninception.py'\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(BNInception, self).__init__()\n",
        "        inplace = True\n",
        "        self.conv1_7x7_s2 = nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
        "        self.conv1_7x7_s2_bn = nn.BatchNorm2d(64, affine=True)\n",
        "        self.conv1_relu_7x7 = nn.ReLU(inplace)\n",
        "        self.pool1_3x3_s2 = nn.MaxPool2d((3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n",
        "        self.conv2_3x3_reduce = nn.Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.conv2_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n",
        "        self.conv2_relu_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.conv2_3x3 = nn.Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.conv2_3x3_bn = nn.BatchNorm2d(192, affine=True)\n",
        "        self.conv2_relu_3x3 = nn.ReLU(inplace)\n",
        "        self.pool2_3x3_s2 = nn.MaxPool2d((3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n",
        "        self.inception_3a_1x1 = nn.Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_3a_1x1_bn = nn.BatchNorm2d(64, affine=True)\n",
        "        self.inception_3a_relu_1x1 = nn.ReLU(inplace)\n",
        "        self.inception_3a_3x3_reduce = nn.Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_3a_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n",
        "        self.inception_3a_relu_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_3a_3x3 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_3a_3x3_bn = nn.BatchNorm2d(64, affine=True)\n",
        "        self.inception_3a_relu_3x3 = nn.ReLU(inplace)\n",
        "        self.inception_3a_double_3x3_reduce = nn.Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_3a_double_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n",
        "        self.inception_3a_relu_double_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_3a_double_3x3_1 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_3a_double_3x3_1_bn = nn.BatchNorm2d(96, affine=True)\n",
        "        self.inception_3a_relu_double_3x3_1 = nn.ReLU(inplace)\n",
        "        self.inception_3a_double_3x3_2 = nn.Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_3a_double_3x3_2_bn = nn.BatchNorm2d(96, affine=True)\n",
        "        self.inception_3a_relu_double_3x3_2 = nn.ReLU(inplace)\n",
        "        self.inception_3a_pool = nn.AvgPool2d(3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n",
        "        self.inception_3a_pool_proj = nn.Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_3a_pool_proj_bn = nn.BatchNorm2d(32, affine=True)\n",
        "        self.inception_3a_relu_pool_proj = nn.ReLU(inplace)\n",
        "        self.inception_3b_1x1 = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_3b_1x1_bn = nn.BatchNorm2d(64, affine=True)\n",
        "        self.inception_3b_relu_1x1 = nn.ReLU(inplace)\n",
        "        self.inception_3b_3x3_reduce = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_3b_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n",
        "        self.inception_3b_relu_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_3b_3x3 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_3b_3x3_bn = nn.BatchNorm2d(96, affine=True)\n",
        "        self.inception_3b_relu_3x3 = nn.ReLU(inplace)\n",
        "        self.inception_3b_double_3x3_reduce = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_3b_double_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n",
        "        self.inception_3b_relu_double_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_3b_double_3x3_1 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_3b_double_3x3_1_bn = nn.BatchNorm2d(96, affine=True)\n",
        "        self.inception_3b_relu_double_3x3_1 = nn.ReLU(inplace)\n",
        "        self.inception_3b_double_3x3_2 = nn.Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_3b_double_3x3_2_bn = nn.BatchNorm2d(96, affine=True)\n",
        "        self.inception_3b_relu_double_3x3_2 = nn.ReLU(inplace)\n",
        "        self.inception_3b_pool = nn.AvgPool2d(3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n",
        "        self.inception_3b_pool_proj = nn.Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_3b_pool_proj_bn = nn.BatchNorm2d(64, affine=True)\n",
        "        self.inception_3b_relu_pool_proj = nn.ReLU(inplace)\n",
        "        self.inception_3c_3x3_reduce = nn.Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_3c_3x3_reduce_bn = nn.BatchNorm2d(128, affine=True)\n",
        "        self.inception_3c_relu_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_3c_3x3 = nn.Conv2d(128, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.inception_3c_3x3_bn = nn.BatchNorm2d(160, affine=True)\n",
        "        self.inception_3c_relu_3x3 = nn.ReLU(inplace)\n",
        "        self.inception_3c_double_3x3_reduce = nn.Conv2d(320, 64, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_3c_double_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n",
        "        self.inception_3c_relu_double_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_3c_double_3x3_1 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_3c_double_3x3_1_bn = nn.BatchNorm2d(96, affine=True)\n",
        "        self.inception_3c_relu_double_3x3_1 = nn.ReLU(inplace)\n",
        "        self.inception_3c_double_3x3_2 = nn.Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.inception_3c_double_3x3_2_bn = nn.BatchNorm2d(96, affine=True)\n",
        "        self.inception_3c_relu_double_3x3_2 = nn.ReLU(inplace)\n",
        "        self.inception_3c_pool = nn.MaxPool2d((3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n",
        "        self.inception_4a_1x1 = nn.Conv2d(576, 224, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_4a_1x1_bn = nn.BatchNorm2d(224, affine=True)\n",
        "        self.inception_4a_relu_1x1 = nn.ReLU(inplace)\n",
        "        self.inception_4a_3x3_reduce = nn.Conv2d(576, 64, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_4a_3x3_reduce_bn = nn.BatchNorm2d(64, affine=True)\n",
        "        self.inception_4a_relu_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_4a_3x3 = nn.Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_4a_3x3_bn = nn.BatchNorm2d(96, affine=True)\n",
        "        self.inception_4a_relu_3x3 = nn.ReLU(inplace)\n",
        "        self.inception_4a_double_3x3_reduce = nn.Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_4a_double_3x3_reduce_bn = nn.BatchNorm2d(96, affine=True)\n",
        "        self.inception_4a_relu_double_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_4a_double_3x3_1 = nn.Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_4a_double_3x3_1_bn = nn.BatchNorm2d(128, affine=True)\n",
        "        self.inception_4a_relu_double_3x3_1 = nn.ReLU(inplace)\n",
        "        self.inception_4a_double_3x3_2 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_4a_double_3x3_2_bn = nn.BatchNorm2d(128, affine=True)\n",
        "        self.inception_4a_relu_double_3x3_2 = nn.ReLU(inplace)\n",
        "        self.inception_4a_pool = nn.AvgPool2d(3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n",
        "        self.inception_4a_pool_proj = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_4a_pool_proj_bn = nn.BatchNorm2d(128, affine=True)\n",
        "        self.inception_4a_relu_pool_proj = nn.ReLU(inplace)\n",
        "        self.inception_4b_1x1 = nn.Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_4b_1x1_bn = nn.BatchNorm2d(192, affine=True)\n",
        "        self.inception_4b_relu_1x1 = nn.ReLU(inplace)\n",
        "        self.inception_4b_3x3_reduce = nn.Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_4b_3x3_reduce_bn = nn.BatchNorm2d(96, affine=True)\n",
        "        self.inception_4b_relu_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_4b_3x3 = nn.Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_4b_3x3_bn = nn.BatchNorm2d(128, affine=True)\n",
        "        self.inception_4b_relu_3x3 = nn.ReLU(inplace)\n",
        "        self.inception_4b_double_3x3_reduce = nn.Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_4b_double_3x3_reduce_bn = nn.BatchNorm2d(96, affine=True)\n",
        "        self.inception_4b_relu_double_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_4b_double_3x3_1 = nn.Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_4b_double_3x3_1_bn = nn.BatchNorm2d(128, affine=True)\n",
        "        self.inception_4b_relu_double_3x3_1 = nn.ReLU(inplace)\n",
        "        self.inception_4b_double_3x3_2 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_4b_double_3x3_2_bn = nn.BatchNorm2d(128, affine=True)\n",
        "        self.inception_4b_relu_double_3x3_2 = nn.ReLU(inplace)\n",
        "        self.inception_4b_pool = nn.AvgPool2d(3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n",
        "        self.inception_4b_pool_proj = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_4b_pool_proj_bn = nn.BatchNorm2d(128, affine=True)\n",
        "        self.inception_4b_relu_pool_proj = nn.ReLU(inplace)\n",
        "        self.inception_4c_1x1 = nn.Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_4c_1x1_bn = nn.BatchNorm2d(160, affine=True)\n",
        "        self.inception_4c_relu_1x1 = nn.ReLU(inplace)\n",
        "        self.inception_4c_3x3_reduce = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_4c_3x3_reduce_bn = nn.BatchNorm2d(128, affine=True)\n",
        "        self.inception_4c_relu_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_4c_3x3 = nn.Conv2d(128, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_4c_3x3_bn = nn.BatchNorm2d(160, affine=True)\n",
        "        self.inception_4c_relu_3x3 = nn.ReLU(inplace)\n",
        "        self.inception_4c_double_3x3_reduce = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_4c_double_3x3_reduce_bn = nn.BatchNorm2d(128, affine=True)\n",
        "        self.inception_4c_relu_double_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_4c_double_3x3_1 = nn.Conv2d(128, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_4c_double_3x3_1_bn = nn.BatchNorm2d(160, affine=True)\n",
        "        self.inception_4c_relu_double_3x3_1 = nn.ReLU(inplace)\n",
        "        self.inception_4c_double_3x3_2 = nn.Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_4c_double_3x3_2_bn = nn.BatchNorm2d(160, affine=True)\n",
        "        self.inception_4c_relu_double_3x3_2 = nn.ReLU(inplace)\n",
        "        self.inception_4c_pool = nn.AvgPool2d(3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n",
        "        self.inception_4c_pool_proj = nn.Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_4c_pool_proj_bn = nn.BatchNorm2d(128, affine=True)\n",
        "        self.inception_4c_relu_pool_proj = nn.ReLU(inplace)\n",
        "        self.inception_4d_1x1 = nn.Conv2d(608, 96, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_4d_1x1_bn = nn.BatchNorm2d(96, affine=True)\n",
        "        self.inception_4d_relu_1x1 = nn.ReLU(inplace)\n",
        "        self.inception_4d_3x3_reduce = nn.Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_4d_3x3_reduce_bn = nn.BatchNorm2d(128, affine=True)\n",
        "        self.inception_4d_relu_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_4d_3x3 = nn.Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_4d_3x3_bn = nn.BatchNorm2d(192, affine=True)\n",
        "        self.inception_4d_relu_3x3 = nn.ReLU(inplace)\n",
        "        self.inception_4d_double_3x3_reduce = nn.Conv2d(608, 160, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_4d_double_3x3_reduce_bn = nn.BatchNorm2d(160, affine=True)\n",
        "        self.inception_4d_relu_double_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_4d_double_3x3_1 = nn.Conv2d(160, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_4d_double_3x3_1_bn = nn.BatchNorm2d(192, affine=True)\n",
        "        self.inception_4d_relu_double_3x3_1 = nn.ReLU(inplace)\n",
        "        self.inception_4d_double_3x3_2 = nn.Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_4d_double_3x3_2_bn = nn.BatchNorm2d(192, affine=True)\n",
        "        self.inception_4d_relu_double_3x3_2 = nn.ReLU(inplace)\n",
        "        self.inception_4d_pool = nn.AvgPool2d(3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n",
        "        self.inception_4d_pool_proj = nn.Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_4d_pool_proj_bn = nn.BatchNorm2d(128, affine=True)\n",
        "        self.inception_4d_relu_pool_proj = nn.ReLU(inplace)\n",
        "        self.inception_4e_3x3_reduce = nn.Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_4e_3x3_reduce_bn = nn.BatchNorm2d(128, affine=True)\n",
        "        self.inception_4e_relu_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_4e_3x3 = nn.Conv2d(128, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.inception_4e_3x3_bn = nn.BatchNorm2d(192, affine=True)\n",
        "        self.inception_4e_relu_3x3 = nn.ReLU(inplace)\n",
        "        self.inception_4e_double_3x3_reduce = nn.Conv2d(608, 192, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_4e_double_3x3_reduce_bn = nn.BatchNorm2d(192, affine=True)\n",
        "        self.inception_4e_relu_double_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_4e_double_3x3_1 = nn.Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_4e_double_3x3_1_bn = nn.BatchNorm2d(256, affine=True)\n",
        "        self.inception_4e_relu_double_3x3_1 = nn.ReLU(inplace)\n",
        "        self.inception_4e_double_3x3_2 = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
        "        self.inception_4e_double_3x3_2_bn = nn.BatchNorm2d(256, affine=True)\n",
        "        self.inception_4e_relu_double_3x3_2 = nn.ReLU(inplace)\n",
        "        self.inception_4e_pool = nn.MaxPool2d((3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n",
        "        self.inception_5a_1x1 = nn.Conv2d(1056, 352, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_5a_1x1_bn = nn.BatchNorm2d(352, affine=True)\n",
        "        self.inception_5a_relu_1x1 = nn.ReLU(inplace)\n",
        "        self.inception_5a_3x3_reduce = nn.Conv2d(1056, 192, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_5a_3x3_reduce_bn = nn.BatchNorm2d(192, affine=True)\n",
        "        self.inception_5a_relu_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_5a_3x3 = nn.Conv2d(192, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_5a_3x3_bn = nn.BatchNorm2d(320, affine=True)\n",
        "        self.inception_5a_relu_3x3 = nn.ReLU(inplace)\n",
        "        self.inception_5a_double_3x3_reduce = nn.Conv2d(1056, 160, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_5a_double_3x3_reduce_bn = nn.BatchNorm2d(160, affine=True)\n",
        "        self.inception_5a_relu_double_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_5a_double_3x3_1 = nn.Conv2d(160, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_5a_double_3x3_1_bn = nn.BatchNorm2d(224, affine=True)\n",
        "        self.inception_5a_relu_double_3x3_1 = nn.ReLU(inplace)\n",
        "        self.inception_5a_double_3x3_2 = nn.Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_5a_double_3x3_2_bn = nn.BatchNorm2d(224, affine=True)\n",
        "        self.inception_5a_relu_double_3x3_2 = nn.ReLU(inplace)\n",
        "        self.inception_5a_pool = nn.AvgPool2d(3, stride=1, padding=1, ceil_mode=True, count_include_pad=True)\n",
        "        self.inception_5a_pool_proj = nn.Conv2d(1056, 128, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_5a_pool_proj_bn = nn.BatchNorm2d(128, affine=True)\n",
        "        self.inception_5a_relu_pool_proj = nn.ReLU(inplace)\n",
        "        self.inception_5b_1x1 = nn.Conv2d(1024, 352, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_5b_1x1_bn = nn.BatchNorm2d(352, affine=True)\n",
        "        self.inception_5b_relu_1x1 = nn.ReLU(inplace)\n",
        "        self.inception_5b_3x3_reduce = nn.Conv2d(1024, 192, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_5b_3x3_reduce_bn = nn.BatchNorm2d(192, affine=True)\n",
        "        self.inception_5b_relu_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_5b_3x3 = nn.Conv2d(192, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_5b_3x3_bn = nn.BatchNorm2d(320, affine=True)\n",
        "        self.inception_5b_relu_3x3 = nn.ReLU(inplace)\n",
        "        self.inception_5b_double_3x3_reduce = nn.Conv2d(1024, 192, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_5b_double_3x3_reduce_bn = nn.BatchNorm2d(192, affine=True)\n",
        "        self.inception_5b_relu_double_3x3_reduce = nn.ReLU(inplace)\n",
        "        self.inception_5b_double_3x3_1 = nn.Conv2d(192, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_5b_double_3x3_1_bn = nn.BatchNorm2d(224, affine=True)\n",
        "        self.inception_5b_relu_double_3x3_1 = nn.ReLU(inplace)\n",
        "        self.inception_5b_double_3x3_2 = nn.Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
        "        self.inception_5b_double_3x3_2_bn = nn.BatchNorm2d(224, affine=True)\n",
        "        self.inception_5b_relu_double_3x3_2 = nn.ReLU(inplace)\n",
        "        self.inception_5b_pool = nn.MaxPool2d((3, 3), stride=(1, 1), padding=(1, 1), dilation=(1, 1), ceil_mode=True)\n",
        "        self.inception_5b_pool_proj = nn.Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1))\n",
        "        self.inception_5b_pool_proj_bn = nn.BatchNorm2d(128, affine=True)\n",
        "        self.inception_5b_relu_pool_proj = nn.ReLU(inplace)\n",
        "\n",
        "    def features(self, input):\n",
        "        conv1_7x7_s2_out = self.conv1_7x7_s2(input)\n",
        "        conv1_7x7_s2_bn_out = self.conv1_7x7_s2_bn(conv1_7x7_s2_out)\n",
        "        conv1_relu_7x7_out = self.conv1_relu_7x7(conv1_7x7_s2_bn_out)\n",
        "        pool1_3x3_s2_out = self.pool1_3x3_s2(conv1_relu_7x7_out)\n",
        "        conv2_3x3_reduce_out = self.conv2_3x3_reduce(pool1_3x3_s2_out)\n",
        "        conv2_3x3_reduce_bn_out = self.conv2_3x3_reduce_bn(conv2_3x3_reduce_out)\n",
        "        conv2_relu_3x3_reduce_out = self.conv2_relu_3x3_reduce(conv2_3x3_reduce_bn_out)\n",
        "        conv2_3x3_out = self.conv2_3x3(conv2_relu_3x3_reduce_out)\n",
        "        conv2_3x3_bn_out = self.conv2_3x3_bn(conv2_3x3_out)\n",
        "        conv2_relu_3x3_out = self.conv2_relu_3x3(conv2_3x3_bn_out)\n",
        "        pool2_3x3_s2_out = self.pool2_3x3_s2(conv2_relu_3x3_out)\n",
        "        inception_3a_1x1_out = self.inception_3a_1x1(pool2_3x3_s2_out)\n",
        "        inception_3a_1x1_bn_out = self.inception_3a_1x1_bn(inception_3a_1x1_out)\n",
        "        inception_3a_relu_1x1_out = self.inception_3a_relu_1x1(inception_3a_1x1_bn_out)\n",
        "        inception_3a_3x3_reduce_out = self.inception_3a_3x3_reduce(pool2_3x3_s2_out)\n",
        "        inception_3a_3x3_reduce_bn_out = self.inception_3a_3x3_reduce_bn(inception_3a_3x3_reduce_out)\n",
        "        inception_3a_relu_3x3_reduce_out = self.inception_3a_relu_3x3_reduce(inception_3a_3x3_reduce_bn_out)\n",
        "        inception_3a_3x3_out = self.inception_3a_3x3(inception_3a_relu_3x3_reduce_out)\n",
        "        inception_3a_3x3_bn_out = self.inception_3a_3x3_bn(inception_3a_3x3_out)\n",
        "        inception_3a_relu_3x3_out = self.inception_3a_relu_3x3(inception_3a_3x3_bn_out)\n",
        "        inception_3a_double_3x3_reduce_out = self.inception_3a_double_3x3_reduce(pool2_3x3_s2_out)\n",
        "        inception_3a_double_3x3_reduce_bn_out = self.inception_3a_double_3x3_reduce_bn(inception_3a_double_3x3_reduce_out)\n",
        "        inception_3a_relu_double_3x3_reduce_out = self.inception_3a_relu_double_3x3_reduce(inception_3a_double_3x3_reduce_bn_out)\n",
        "        inception_3a_double_3x3_1_out = self.inception_3a_double_3x3_1(inception_3a_relu_double_3x3_reduce_out)\n",
        "        inception_3a_double_3x3_1_bn_out = self.inception_3a_double_3x3_1_bn(inception_3a_double_3x3_1_out)\n",
        "        inception_3a_relu_double_3x3_1_out = self.inception_3a_relu_double_3x3_1(inception_3a_double_3x3_1_bn_out)\n",
        "        inception_3a_double_3x3_2_out = self.inception_3a_double_3x3_2(inception_3a_relu_double_3x3_1_out)\n",
        "        inception_3a_double_3x3_2_bn_out = self.inception_3a_double_3x3_2_bn(inception_3a_double_3x3_2_out)\n",
        "        inception_3a_relu_double_3x3_2_out = self.inception_3a_relu_double_3x3_2(inception_3a_double_3x3_2_bn_out)\n",
        "        inception_3a_pool_out = self.inception_3a_pool(pool2_3x3_s2_out)\n",
        "        inception_3a_pool_proj_out = self.inception_3a_pool_proj(inception_3a_pool_out)\n",
        "        inception_3a_pool_proj_bn_out = self.inception_3a_pool_proj_bn(inception_3a_pool_proj_out)\n",
        "        inception_3a_relu_pool_proj_out = self.inception_3a_relu_pool_proj(inception_3a_pool_proj_bn_out)\n",
        "        inception_3a_output_out = torch.cat([inception_3a_relu_1x1_out,inception_3a_relu_3x3_out,inception_3a_relu_double_3x3_2_out ,inception_3a_relu_pool_proj_out], 1)\n",
        "        inception_3b_1x1_out = self.inception_3b_1x1(inception_3a_output_out)\n",
        "        inception_3b_1x1_bn_out = self.inception_3b_1x1_bn(inception_3b_1x1_out)\n",
        "        inception_3b_relu_1x1_out = self.inception_3b_relu_1x1(inception_3b_1x1_bn_out)\n",
        "        inception_3b_3x3_reduce_out = self.inception_3b_3x3_reduce(inception_3a_output_out)\n",
        "        inception_3b_3x3_reduce_bn_out = self.inception_3b_3x3_reduce_bn(inception_3b_3x3_reduce_out)\n",
        "        inception_3b_relu_3x3_reduce_out = self.inception_3b_relu_3x3_reduce(inception_3b_3x3_reduce_bn_out)\n",
        "        inception_3b_3x3_out = self.inception_3b_3x3(inception_3b_relu_3x3_reduce_out)\n",
        "        inception_3b_3x3_bn_out = self.inception_3b_3x3_bn(inception_3b_3x3_out)\n",
        "        inception_3b_relu_3x3_out = self.inception_3b_relu_3x3(inception_3b_3x3_bn_out)\n",
        "        inception_3b_double_3x3_reduce_out = self.inception_3b_double_3x3_reduce(inception_3a_output_out)\n",
        "        inception_3b_double_3x3_reduce_bn_out = self.inception_3b_double_3x3_reduce_bn(inception_3b_double_3x3_reduce_out)\n",
        "        inception_3b_relu_double_3x3_reduce_out = self.inception_3b_relu_double_3x3_reduce(inception_3b_double_3x3_reduce_bn_out)\n",
        "        inception_3b_double_3x3_1_out = self.inception_3b_double_3x3_1(inception_3b_relu_double_3x3_reduce_out)\n",
        "        inception_3b_double_3x3_1_bn_out = self.inception_3b_double_3x3_1_bn(inception_3b_double_3x3_1_out)\n",
        "        inception_3b_relu_double_3x3_1_out = self.inception_3b_relu_double_3x3_1(inception_3b_double_3x3_1_bn_out)\n",
        "        inception_3b_double_3x3_2_out = self.inception_3b_double_3x3_2(inception_3b_relu_double_3x3_1_out)\n",
        "        inception_3b_double_3x3_2_bn_out = self.inception_3b_double_3x3_2_bn(inception_3b_double_3x3_2_out)\n",
        "        inception_3b_relu_double_3x3_2_out = self.inception_3b_relu_double_3x3_2(inception_3b_double_3x3_2_bn_out)\n",
        "        inception_3b_pool_out = self.inception_3b_pool(inception_3a_output_out)\n",
        "        inception_3b_pool_proj_out = self.inception_3b_pool_proj(inception_3b_pool_out)\n",
        "        inception_3b_pool_proj_bn_out = self.inception_3b_pool_proj_bn(inception_3b_pool_proj_out)\n",
        "        inception_3b_relu_pool_proj_out = self.inception_3b_relu_pool_proj(inception_3b_pool_proj_bn_out)\n",
        "        inception_3b_output_out = torch.cat([inception_3b_relu_1x1_out,inception_3b_relu_3x3_out,inception_3b_relu_double_3x3_2_out,inception_3b_relu_pool_proj_out], 1)\n",
        "        inception_3c_3x3_reduce_out = self.inception_3c_3x3_reduce(inception_3b_output_out)\n",
        "        inception_3c_3x3_reduce_bn_out = self.inception_3c_3x3_reduce_bn(inception_3c_3x3_reduce_out)\n",
        "        inception_3c_relu_3x3_reduce_out = self.inception_3c_relu_3x3_reduce(inception_3c_3x3_reduce_bn_out)\n",
        "        inception_3c_3x3_out = self.inception_3c_3x3(inception_3c_relu_3x3_reduce_out)\n",
        "        inception_3c_3x3_bn_out = self.inception_3c_3x3_bn(inception_3c_3x3_out)\n",
        "        inception_3c_relu_3x3_out = self.inception_3c_relu_3x3(inception_3c_3x3_bn_out)\n",
        "        inception_3c_double_3x3_reduce_out = self.inception_3c_double_3x3_reduce(inception_3b_output_out)\n",
        "        inception_3c_double_3x3_reduce_bn_out = self.inception_3c_double_3x3_reduce_bn(inception_3c_double_3x3_reduce_out)\n",
        "        inception_3c_relu_double_3x3_reduce_out = self.inception_3c_relu_double_3x3_reduce(inception_3c_double_3x3_reduce_bn_out)\n",
        "        inception_3c_double_3x3_1_out = self.inception_3c_double_3x3_1(inception_3c_relu_double_3x3_reduce_out)\n",
        "        inception_3c_double_3x3_1_bn_out = self.inception_3c_double_3x3_1_bn(inception_3c_double_3x3_1_out)\n",
        "        inception_3c_relu_double_3x3_1_out = self.inception_3c_relu_double_3x3_1(inception_3c_double_3x3_1_bn_out)\n",
        "        inception_3c_double_3x3_2_out = self.inception_3c_double_3x3_2(inception_3c_relu_double_3x3_1_out)\n",
        "        inception_3c_double_3x3_2_bn_out = self.inception_3c_double_3x3_2_bn(inception_3c_double_3x3_2_out)\n",
        "        inception_3c_relu_double_3x3_2_out = self.inception_3c_relu_double_3x3_2(inception_3c_double_3x3_2_bn_out)\n",
        "        inception_3c_pool_out = self.inception_3c_pool(inception_3b_output_out)\n",
        "        inception_3c_output_out = torch.cat([inception_3c_relu_3x3_out,inception_3c_relu_double_3x3_2_out,inception_3c_pool_out], 1)\n",
        "        inception_4a_1x1_out = self.inception_4a_1x1(inception_3c_output_out)\n",
        "        inception_4a_1x1_bn_out = self.inception_4a_1x1_bn(inception_4a_1x1_out)\n",
        "        inception_4a_relu_1x1_out = self.inception_4a_relu_1x1(inception_4a_1x1_bn_out)\n",
        "        inception_4a_3x3_reduce_out = self.inception_4a_3x3_reduce(inception_3c_output_out)\n",
        "        inception_4a_3x3_reduce_bn_out = self.inception_4a_3x3_reduce_bn(inception_4a_3x3_reduce_out)\n",
        "        inception_4a_relu_3x3_reduce_out = self.inception_4a_relu_3x3_reduce(inception_4a_3x3_reduce_bn_out)\n",
        "        inception_4a_3x3_out = self.inception_4a_3x3(inception_4a_relu_3x3_reduce_out)\n",
        "        inception_4a_3x3_bn_out = self.inception_4a_3x3_bn(inception_4a_3x3_out)\n",
        "        inception_4a_relu_3x3_out = self.inception_4a_relu_3x3(inception_4a_3x3_bn_out)\n",
        "        inception_4a_double_3x3_reduce_out = self.inception_4a_double_3x3_reduce(inception_3c_output_out)\n",
        "        inception_4a_double_3x3_reduce_bn_out = self.inception_4a_double_3x3_reduce_bn(inception_4a_double_3x3_reduce_out)\n",
        "        inception_4a_relu_double_3x3_reduce_out = self.inception_4a_relu_double_3x3_reduce(inception_4a_double_3x3_reduce_bn_out)\n",
        "        inception_4a_double_3x3_1_out = self.inception_4a_double_3x3_1(inception_4a_relu_double_3x3_reduce_out)\n",
        "        inception_4a_double_3x3_1_bn_out = self.inception_4a_double_3x3_1_bn(inception_4a_double_3x3_1_out)\n",
        "        inception_4a_relu_double_3x3_1_out = self.inception_4a_relu_double_3x3_1(inception_4a_double_3x3_1_bn_out)\n",
        "        inception_4a_double_3x3_2_out = self.inception_4a_double_3x3_2(inception_4a_relu_double_3x3_1_out)\n",
        "        inception_4a_double_3x3_2_bn_out = self.inception_4a_double_3x3_2_bn(inception_4a_double_3x3_2_out)\n",
        "        inception_4a_relu_double_3x3_2_out = self.inception_4a_relu_double_3x3_2(inception_4a_double_3x3_2_bn_out)\n",
        "        inception_4a_pool_out = self.inception_4a_pool(inception_3c_output_out)\n",
        "        inception_4a_pool_proj_out = self.inception_4a_pool_proj(inception_4a_pool_out)\n",
        "        inception_4a_pool_proj_bn_out = self.inception_4a_pool_proj_bn(inception_4a_pool_proj_out)\n",
        "        inception_4a_relu_pool_proj_out = self.inception_4a_relu_pool_proj(inception_4a_pool_proj_bn_out)\n",
        "        inception_4a_output_out = torch.cat([inception_4a_relu_1x1_out,inception_4a_relu_3x3_out,inception_4a_relu_double_3x3_2_out,inception_4a_relu_pool_proj_out], 1)\n",
        "        inception_4b_1x1_out = self.inception_4b_1x1(inception_4a_output_out)\n",
        "        inception_4b_1x1_bn_out = self.inception_4b_1x1_bn(inception_4b_1x1_out)\n",
        "        inception_4b_relu_1x1_out = self.inception_4b_relu_1x1(inception_4b_1x1_bn_out)\n",
        "        inception_4b_3x3_reduce_out = self.inception_4b_3x3_reduce(inception_4a_output_out)\n",
        "        inception_4b_3x3_reduce_bn_out = self.inception_4b_3x3_reduce_bn(inception_4b_3x3_reduce_out)\n",
        "        inception_4b_relu_3x3_reduce_out = self.inception_4b_relu_3x3_reduce(inception_4b_3x3_reduce_bn_out)\n",
        "        inception_4b_3x3_out = self.inception_4b_3x3(inception_4b_relu_3x3_reduce_out)\n",
        "        inception_4b_3x3_bn_out = self.inception_4b_3x3_bn(inception_4b_3x3_out)\n",
        "        inception_4b_relu_3x3_out = self.inception_4b_relu_3x3(inception_4b_3x3_bn_out)\n",
        "        inception_4b_double_3x3_reduce_out = self.inception_4b_double_3x3_reduce(inception_4a_output_out)\n",
        "        inception_4b_double_3x3_reduce_bn_out = self.inception_4b_double_3x3_reduce_bn(inception_4b_double_3x3_reduce_out)\n",
        "        inception_4b_relu_double_3x3_reduce_out = self.inception_4b_relu_double_3x3_reduce(inception_4b_double_3x3_reduce_bn_out)\n",
        "        inception_4b_double_3x3_1_out = self.inception_4b_double_3x3_1(inception_4b_relu_double_3x3_reduce_out)\n",
        "        inception_4b_double_3x3_1_bn_out = self.inception_4b_double_3x3_1_bn(inception_4b_double_3x3_1_out)\n",
        "        inception_4b_relu_double_3x3_1_out = self.inception_4b_relu_double_3x3_1(inception_4b_double_3x3_1_bn_out)\n",
        "        inception_4b_double_3x3_2_out = self.inception_4b_double_3x3_2(inception_4b_relu_double_3x3_1_out)\n",
        "        inception_4b_double_3x3_2_bn_out = self.inception_4b_double_3x3_2_bn(inception_4b_double_3x3_2_out)\n",
        "        inception_4b_relu_double_3x3_2_out = self.inception_4b_relu_double_3x3_2(inception_4b_double_3x3_2_bn_out)\n",
        "        inception_4b_pool_out = self.inception_4b_pool(inception_4a_output_out)\n",
        "        inception_4b_pool_proj_out = self.inception_4b_pool_proj(inception_4b_pool_out)\n",
        "        inception_4b_pool_proj_bn_out = self.inception_4b_pool_proj_bn(inception_4b_pool_proj_out)\n",
        "        inception_4b_relu_pool_proj_out = self.inception_4b_relu_pool_proj(inception_4b_pool_proj_bn_out)\n",
        "        inception_4b_output_out = torch.cat([inception_4b_relu_1x1_out,inception_4b_relu_3x3_out,inception_4b_relu_double_3x3_2_out,inception_4b_relu_pool_proj_out], 1)\n",
        "        inception_4c_1x1_out = self.inception_4c_1x1(inception_4b_output_out)\n",
        "        inception_4c_1x1_bn_out = self.inception_4c_1x1_bn(inception_4c_1x1_out)\n",
        "        inception_4c_relu_1x1_out = self.inception_4c_relu_1x1(inception_4c_1x1_bn_out)\n",
        "        inception_4c_3x3_reduce_out = self.inception_4c_3x3_reduce(inception_4b_output_out)\n",
        "        inception_4c_3x3_reduce_bn_out = self.inception_4c_3x3_reduce_bn(inception_4c_3x3_reduce_out)\n",
        "        inception_4c_relu_3x3_reduce_out = self.inception_4c_relu_3x3_reduce(inception_4c_3x3_reduce_bn_out)\n",
        "        inception_4c_3x3_out = self.inception_4c_3x3(inception_4c_relu_3x3_reduce_out)\n",
        "        inception_4c_3x3_bn_out = self.inception_4c_3x3_bn(inception_4c_3x3_out)\n",
        "        inception_4c_relu_3x3_out = self.inception_4c_relu_3x3(inception_4c_3x3_bn_out)\n",
        "        inception_4c_double_3x3_reduce_out = self.inception_4c_double_3x3_reduce(inception_4b_output_out)\n",
        "        inception_4c_double_3x3_reduce_bn_out = self.inception_4c_double_3x3_reduce_bn(inception_4c_double_3x3_reduce_out)\n",
        "        inception_4c_relu_double_3x3_reduce_out = self.inception_4c_relu_double_3x3_reduce(inception_4c_double_3x3_reduce_bn_out)\n",
        "        inception_4c_double_3x3_1_out = self.inception_4c_double_3x3_1(inception_4c_relu_double_3x3_reduce_out)\n",
        "        inception_4c_double_3x3_1_bn_out = self.inception_4c_double_3x3_1_bn(inception_4c_double_3x3_1_out)\n",
        "        inception_4c_relu_double_3x3_1_out = self.inception_4c_relu_double_3x3_1(inception_4c_double_3x3_1_bn_out)\n",
        "        inception_4c_double_3x3_2_out = self.inception_4c_double_3x3_2(inception_4c_relu_double_3x3_1_out)\n",
        "        inception_4c_double_3x3_2_bn_out = self.inception_4c_double_3x3_2_bn(inception_4c_double_3x3_2_out)\n",
        "        inception_4c_relu_double_3x3_2_out = self.inception_4c_relu_double_3x3_2(inception_4c_double_3x3_2_bn_out)\n",
        "        inception_4c_pool_out = self.inception_4c_pool(inception_4b_output_out)\n",
        "        inception_4c_pool_proj_out = self.inception_4c_pool_proj(inception_4c_pool_out)\n",
        "        inception_4c_pool_proj_bn_out = self.inception_4c_pool_proj_bn(inception_4c_pool_proj_out)\n",
        "        inception_4c_relu_pool_proj_out = self.inception_4c_relu_pool_proj(inception_4c_pool_proj_bn_out)\n",
        "        inception_4c_output_out = torch.cat([inception_4c_relu_1x1_out,inception_4c_relu_3x3_out,inception_4c_relu_double_3x3_2_out,inception_4c_relu_pool_proj_out], 1)\n",
        "        inception_4d_1x1_out = self.inception_4d_1x1(inception_4c_output_out)\n",
        "        inception_4d_1x1_bn_out = self.inception_4d_1x1_bn(inception_4d_1x1_out)\n",
        "        inception_4d_relu_1x1_out = self.inception_4d_relu_1x1(inception_4d_1x1_bn_out)\n",
        "        inception_4d_3x3_reduce_out = self.inception_4d_3x3_reduce(inception_4c_output_out)\n",
        "        inception_4d_3x3_reduce_bn_out = self.inception_4d_3x3_reduce_bn(inception_4d_3x3_reduce_out)\n",
        "        inception_4d_relu_3x3_reduce_out = self.inception_4d_relu_3x3_reduce(inception_4d_3x3_reduce_bn_out)\n",
        "        inception_4d_3x3_out = self.inception_4d_3x3(inception_4d_relu_3x3_reduce_out)\n",
        "        inception_4d_3x3_bn_out = self.inception_4d_3x3_bn(inception_4d_3x3_out)\n",
        "        inception_4d_relu_3x3_out = self.inception_4d_relu_3x3(inception_4d_3x3_bn_out)\n",
        "        inception_4d_double_3x3_reduce_out = self.inception_4d_double_3x3_reduce(inception_4c_output_out)\n",
        "        inception_4d_double_3x3_reduce_bn_out = self.inception_4d_double_3x3_reduce_bn(inception_4d_double_3x3_reduce_out)\n",
        "        inception_4d_relu_double_3x3_reduce_out = self.inception_4d_relu_double_3x3_reduce(inception_4d_double_3x3_reduce_bn_out)\n",
        "        inception_4d_double_3x3_1_out = self.inception_4d_double_3x3_1(inception_4d_relu_double_3x3_reduce_out)\n",
        "        inception_4d_double_3x3_1_bn_out = self.inception_4d_double_3x3_1_bn(inception_4d_double_3x3_1_out)\n",
        "        inception_4d_relu_double_3x3_1_out = self.inception_4d_relu_double_3x3_1(inception_4d_double_3x3_1_bn_out)\n",
        "        inception_4d_double_3x3_2_out = self.inception_4d_double_3x3_2(inception_4d_relu_double_3x3_1_out)\n",
        "        inception_4d_double_3x3_2_bn_out = self.inception_4d_double_3x3_2_bn(inception_4d_double_3x3_2_out)\n",
        "        inception_4d_relu_double_3x3_2_out = self.inception_4d_relu_double_3x3_2(inception_4d_double_3x3_2_bn_out)\n",
        "        inception_4d_pool_out = self.inception_4d_pool(inception_4c_output_out)\n",
        "        inception_4d_pool_proj_out = self.inception_4d_pool_proj(inception_4d_pool_out)\n",
        "        inception_4d_pool_proj_bn_out = self.inception_4d_pool_proj_bn(inception_4d_pool_proj_out)\n",
        "        inception_4d_relu_pool_proj_out = self.inception_4d_relu_pool_proj(inception_4d_pool_proj_bn_out)\n",
        "        inception_4d_output_out = torch.cat([inception_4d_relu_1x1_out,inception_4d_relu_3x3_out,inception_4d_relu_double_3x3_2_out,inception_4d_relu_pool_proj_out], 1)\n",
        "        inception_4e_3x3_reduce_out = self.inception_4e_3x3_reduce(inception_4d_output_out)\n",
        "        inception_4e_3x3_reduce_bn_out = self.inception_4e_3x3_reduce_bn(inception_4e_3x3_reduce_out)\n",
        "        inception_4e_relu_3x3_reduce_out = self.inception_4e_relu_3x3_reduce(inception_4e_3x3_reduce_bn_out)\n",
        "        inception_4e_3x3_out = self.inception_4e_3x3(inception_4e_relu_3x3_reduce_out)\n",
        "        inception_4e_3x3_bn_out = self.inception_4e_3x3_bn(inception_4e_3x3_out)\n",
        "        inception_4e_relu_3x3_out = self.inception_4e_relu_3x3(inception_4e_3x3_bn_out)\n",
        "        inception_4e_double_3x3_reduce_out = self.inception_4e_double_3x3_reduce(inception_4d_output_out)\n",
        "        inception_4e_double_3x3_reduce_bn_out = self.inception_4e_double_3x3_reduce_bn(inception_4e_double_3x3_reduce_out)\n",
        "        inception_4e_relu_double_3x3_reduce_out = self.inception_4e_relu_double_3x3_reduce(inception_4e_double_3x3_reduce_bn_out)\n",
        "        inception_4e_double_3x3_1_out = self.inception_4e_double_3x3_1(inception_4e_relu_double_3x3_reduce_out)\n",
        "        inception_4e_double_3x3_1_bn_out = self.inception_4e_double_3x3_1_bn(inception_4e_double_3x3_1_out)\n",
        "        inception_4e_relu_double_3x3_1_out = self.inception_4e_relu_double_3x3_1(inception_4e_double_3x3_1_bn_out)\n",
        "        inception_4e_double_3x3_2_out = self.inception_4e_double_3x3_2(inception_4e_relu_double_3x3_1_out)\n",
        "        inception_4e_double_3x3_2_bn_out = self.inception_4e_double_3x3_2_bn(inception_4e_double_3x3_2_out)\n",
        "        inception_4e_relu_double_3x3_2_out = self.inception_4e_relu_double_3x3_2(inception_4e_double_3x3_2_bn_out)\n",
        "        inception_4e_pool_out = self.inception_4e_pool(inception_4d_output_out)\n",
        "        inception_4e_output_out = torch.cat([inception_4e_relu_3x3_out,inception_4e_relu_double_3x3_2_out,inception_4e_pool_out], 1)\n",
        "        inception_5a_1x1_out = self.inception_5a_1x1(inception_4e_output_out)\n",
        "        inception_5a_1x1_bn_out = self.inception_5a_1x1_bn(inception_5a_1x1_out)\n",
        "        inception_5a_relu_1x1_out = self.inception_5a_relu_1x1(inception_5a_1x1_bn_out)\n",
        "        inception_5a_3x3_reduce_out = self.inception_5a_3x3_reduce(inception_4e_output_out)\n",
        "        inception_5a_3x3_reduce_bn_out = self.inception_5a_3x3_reduce_bn(inception_5a_3x3_reduce_out)\n",
        "        inception_5a_relu_3x3_reduce_out = self.inception_5a_relu_3x3_reduce(inception_5a_3x3_reduce_bn_out)\n",
        "        inception_5a_3x3_out = self.inception_5a_3x3(inception_5a_relu_3x3_reduce_out)\n",
        "        inception_5a_3x3_bn_out = self.inception_5a_3x3_bn(inception_5a_3x3_out)\n",
        "        inception_5a_relu_3x3_out = self.inception_5a_relu_3x3(inception_5a_3x3_bn_out)\n",
        "        inception_5a_double_3x3_reduce_out = self.inception_5a_double_3x3_reduce(inception_4e_output_out)\n",
        "        inception_5a_double_3x3_reduce_bn_out = self.inception_5a_double_3x3_reduce_bn(inception_5a_double_3x3_reduce_out)\n",
        "        inception_5a_relu_double_3x3_reduce_out = self.inception_5a_relu_double_3x3_reduce(inception_5a_double_3x3_reduce_bn_out)\n",
        "        inception_5a_double_3x3_1_out = self.inception_5a_double_3x3_1(inception_5a_relu_double_3x3_reduce_out)\n",
        "        inception_5a_double_3x3_1_bn_out = self.inception_5a_double_3x3_1_bn(inception_5a_double_3x3_1_out)\n",
        "        inception_5a_relu_double_3x3_1_out = self.inception_5a_relu_double_3x3_1(inception_5a_double_3x3_1_bn_out)\n",
        "        inception_5a_double_3x3_2_out = self.inception_5a_double_3x3_2(inception_5a_relu_double_3x3_1_out)\n",
        "        inception_5a_double_3x3_2_bn_out = self.inception_5a_double_3x3_2_bn(inception_5a_double_3x3_2_out)\n",
        "        inception_5a_relu_double_3x3_2_out = self.inception_5a_relu_double_3x3_2(inception_5a_double_3x3_2_bn_out)\n",
        "        inception_5a_pool_out = self.inception_5a_pool(inception_4e_output_out)\n",
        "        inception_5a_pool_proj_out = self.inception_5a_pool_proj(inception_5a_pool_out)\n",
        "        inception_5a_pool_proj_bn_out = self.inception_5a_pool_proj_bn(inception_5a_pool_proj_out)\n",
        "        inception_5a_relu_pool_proj_out = self.inception_5a_relu_pool_proj(inception_5a_pool_proj_bn_out)\n",
        "        inception_5a_output_out = torch.cat([inception_5a_relu_1x1_out,inception_5a_relu_3x3_out,inception_5a_relu_double_3x3_2_out,inception_5a_relu_pool_proj_out], 1)\n",
        "        inception_5b_1x1_out = self.inception_5b_1x1(inception_5a_output_out)\n",
        "        inception_5b_1x1_bn_out = self.inception_5b_1x1_bn(inception_5b_1x1_out)\n",
        "        inception_5b_relu_1x1_out = self.inception_5b_relu_1x1(inception_5b_1x1_bn_out)\n",
        "        inception_5b_3x3_reduce_out = self.inception_5b_3x3_reduce(inception_5a_output_out)\n",
        "        inception_5b_3x3_reduce_bn_out = self.inception_5b_3x3_reduce_bn(inception_5b_3x3_reduce_out)\n",
        "        inception_5b_relu_3x3_reduce_out = self.inception_5b_relu_3x3_reduce(inception_5b_3x3_reduce_bn_out)\n",
        "        inception_5b_3x3_out = self.inception_5b_3x3(inception_5b_relu_3x3_reduce_out)\n",
        "        inception_5b_3x3_bn_out = self.inception_5b_3x3_bn(inception_5b_3x3_out)\n",
        "        inception_5b_relu_3x3_out = self.inception_5b_relu_3x3(inception_5b_3x3_bn_out)\n",
        "        inception_5b_double_3x3_reduce_out = self.inception_5b_double_3x3_reduce(inception_5a_output_out)\n",
        "        inception_5b_double_3x3_reduce_bn_out = self.inception_5b_double_3x3_reduce_bn(inception_5b_double_3x3_reduce_out)\n",
        "        inception_5b_relu_double_3x3_reduce_out = self.inception_5b_relu_double_3x3_reduce(inception_5b_double_3x3_reduce_bn_out)\n",
        "        inception_5b_double_3x3_1_out = self.inception_5b_double_3x3_1(inception_5b_relu_double_3x3_reduce_out)\n",
        "        inception_5b_double_3x3_1_bn_out = self.inception_5b_double_3x3_1_bn(inception_5b_double_3x3_1_out)\n",
        "        inception_5b_relu_double_3x3_1_out = self.inception_5b_relu_double_3x3_1(inception_5b_double_3x3_1_bn_out)\n",
        "        inception_5b_double_3x3_2_out = self.inception_5b_double_3x3_2(inception_5b_relu_double_3x3_1_out)\n",
        "        inception_5b_double_3x3_2_bn_out = self.inception_5b_double_3x3_2_bn(inception_5b_double_3x3_2_out)\n",
        "        inception_5b_relu_double_3x3_2_out = self.inception_5b_relu_double_3x3_2(inception_5b_double_3x3_2_bn_out)\n",
        "        inception_5b_pool_out = self.inception_5b_pool(inception_5a_output_out)\n",
        "        inception_5b_pool_proj_out = self.inception_5b_pool_proj(inception_5b_pool_out)\n",
        "        inception_5b_pool_proj_bn_out = self.inception_5b_pool_proj_bn(inception_5b_pool_proj_out)\n",
        "        inception_5b_relu_pool_proj_out = self.inception_5b_relu_pool_proj(inception_5b_pool_proj_bn_out)\n",
        "        inception_5b_output_out = torch.cat([inception_5b_relu_1x1_out,inception_5b_relu_3x3_out,inception_5b_relu_double_3x3_2_out,inception_5b_relu_pool_proj_out], 1)\n",
        "        return inception_3b_output_out,inception_4d_output_out,inception_5b_output_out\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.features(input)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfK1obJr3Gx_"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/model.py\",'w') as f:\n",
        "  f.write(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bTz3KbU5m0C"
      },
      "source": [
        "## Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGDiBqLV3Mqz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models import resnet18\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from model import InceptionNet\n",
        "\n",
        "# Load pre-trained models\n",
        "# Person Detection Model\n",
        "person_detector = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "person_detector.eval()\n",
        "\n",
        "# Define image transformations\n",
        "person_detector_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)  # Print the device being used\n",
        "\n",
        "# Load the pretrained model\n",
        "gender_classifier = InceptionNet(num_classes=35)  # num_classes should match the PETA dataset\n",
        "checkpoint = torch.load('/content/peta_epoch_31.pth.tar', map_location=device)  # Load to GPU if available\n",
        "\n",
        "# Modify the keys in the checkpoint to remove the 'module.' prefix\n",
        "new_state_dict = {key.replace(\"module.\", \"\"): value for key, value in checkpoint['state_dict'].items()}\n",
        "\n",
        "# Load the modified state_dict into the model\n",
        "gender_classifier.load_state_dict(new_state_dict)\n",
        "gender_classifier.to(device)  # Move the model to the GPU\n",
        "\n",
        "gender_classifier.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Define the transformations as per the training setup\n",
        "gender_classifier_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(256, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcXlvqRx6FZk"
      },
      "source": [
        "## Gender Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRWOcDdc5tGB"
      },
      "outputs": [],
      "source": [
        "def gender_classifier_predict(image):\n",
        "    \"\"\"\n",
        "    Perform inference on a single image and print results.\n",
        "    :param image_path: Path to the image file\n",
        "    \"\"\"\n",
        "    input_batch = gender_classifier_transform(image).unsqueeze(0).to(device)  # Move input to GPU\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():  # Disable gradient calculation for inference\n",
        "        output = gender_classifier(input_batch)\n",
        "\n",
        "    # Convert output to probabilities if needed\n",
        "    probabilities = torch.sigmoid(output[0]).squeeze().cpu().numpy()  # Assuming multi-label classification\n",
        "\n",
        "    # Get attribute names based on PETA dataset\n",
        "    peta_attributes = [\n",
        "        'Age16-30', 'Age31-45', 'Age46-60', 'AgeAbove61', 'Backpack', 'CarryingOther',\n",
        "        'Casual lower', 'Casual upper', 'Formal lower', 'Formal upper', 'Hat', 'Jacket',\n",
        "        'Jeans', 'Leather Shoes', 'Logo', 'Long hair', 'Male', 'Messenger Bag', 'Muffler',\n",
        "        'No accessory', 'No carrying', 'Plaid', 'PlasticBags', 'Sandals', 'Shoes', 'Shorts',\n",
        "        'Short Sleeve', 'Skirt', 'Sneaker', 'Stripes', 'Sunglasses', 'Trousers', 'Tshirt',\n",
        "        'UpperOther', 'V-Neck'\n",
        "    ]\n",
        "\n",
        "    # Print out the predictions\n",
        "    threshold = 0.5  # Adjust threshold if needed\n",
        "    predicted_attributes = [peta_attributes[i] for i, prob in enumerate(probabilities) if prob > threshold]\n",
        "\n",
        "    print(\"Predicted Attributes: \",predicted_attributes)\n",
        "    if \"Male\" in predicted_attributes:\n",
        "        return \"Male\"\n",
        "    else:\n",
        "        return \"Female\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjB8KIY56nT0"
      },
      "source": [
        "## Person Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fp8R4Lq15zjO"
      },
      "outputs": [],
      "source": [
        "# Function to perform person detection\n",
        "def detect_person(image):\n",
        "    with torch.no_grad():\n",
        "        predictions = person_detector([image])\n",
        "    return predictions[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjgFh638DRL9"
      },
      "source": [
        "## Frame by Frame Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq0drv166zNs"
      },
      "outputs": [],
      "source": [
        "def process_frame(frame):\n",
        "    # Convert to PIL Image and apply transforms\n",
        "    image = transforms.ToTensor()(frame)\n",
        "\n",
        "    # Detect persons in the frame\n",
        "    detections = detect_person(image)\n",
        "\n",
        "    for i, box in enumerate(detections['boxes']):\n",
        "        if detections['labels'][i] == 1 and detections['scores'][i] > 0.8:  # Label 1 corresponds to 'person'\n",
        "            # Extract bounding box coordinates\n",
        "            xmin, ymin, xmax, ymax = map(int, box)\n",
        "\n",
        "            # Crop the image around the detected person\n",
        "            cropped_image = to_pil_image(image[:, ymin:ymax, xmin:xmax])\n",
        "\n",
        "            # Classify gender\n",
        "            gender = gender_classifier_predict(cropped_image)\n",
        "\n",
        "            # Draw bounding box and label on the frame\n",
        "            color = (0, 255, 0) if gender == \"Male\" else (0, 0, 255)\n",
        "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n",
        "            cv2.putText(frame, gender, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
        "\n",
        "    return frame\n",
        "\n",
        "# Main function to read and process video footage\n",
        "def main():\n",
        "    # Video capture from CCTV footage\n",
        "    cap = cv2.VideoCapture('/content/VIRAT_S_010204_05_000856_000890.mp4')  # Replace with your video file path or camera feed\n",
        "\n",
        "    frame_skip = 30  # Number of frames to skip\n",
        "    frame_count = 0  # Initialize frame count\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_count % frame_skip == 0:\n",
        "            # Process the frame to detect and classify persons\n",
        "            processed_frame = process_frame(frame)\n",
        "\n",
        "            # Display the processed frame\n",
        "            cv2_imshow(processed_frame)\n",
        "\n",
        "        frame_count += 1\n",
        "        # Break loop on 'q' key press\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkTx0hTbDMh9"
      },
      "source": [
        "## Output.mp4 implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d73nLMdfAtKt"
      },
      "outputs": [],
      "source": [
        "def process_frame(frame):\n",
        "    # Convert to PIL Image and apply transforms\n",
        "    image = transforms.ToTensor()(frame)\n",
        "\n",
        "    # Detect persons in the frame\n",
        "    detections = detect_person(image)\n",
        "\n",
        "    for i, box in enumerate(detections['boxes']):\n",
        "        if detections['labels'][i] == 1 and detections['scores'][i] > 0.8:  # Label 1 corresponds to 'person'\n",
        "            # Extract bounding box coordinates\n",
        "            xmin, ymin, xmax, ymax = map(int, box)\n",
        "\n",
        "            # Crop the image around the detected person\n",
        "            cropped_image = to_pil_image(image[:, ymin:ymax, xmin:xmax])\n",
        "\n",
        "            # Classify gender\n",
        "            gender = gender_classifier_predict(cropped_image)\n",
        "\n",
        "            # Draw bounding box and label on the frame\n",
        "            color = (0, 255, 0) if gender == \"Male\" else (0, 0, 255)\n",
        "            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n",
        "            cv2.putText(frame, gender, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
        "\n",
        "    return frame\n",
        "\n",
        "# Main function to read, process video footage, and save to output video\n",
        "def main():\n",
        "    # Video capture from CCTV footage\n",
        "    cap = cv2.VideoCapture('/content/outside.mp4')  # Replace with your video file path or camera feed\n",
        "\n",
        "    # Get frame width, height, and FPS of the input video\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Define the codec and create VideoWriter object\n",
        "    out = cv2.VideoWriter('output_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
        "\n",
        "    frame_skip = 30  # Number of frames to skip\n",
        "    frame_count = 0  # Initialize frame count\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_count % frame_skip == 0:\n",
        "            # Process the frame to detect and classify persons\n",
        "            processed_frame = process_frame(frame)\n",
        "\n",
        "            # Write the processed frame to the output video\n",
        "            out.write(processed_frame)\n",
        "\n",
        "        frame_count += 1\n",
        "        # Break loop on 'q' key press\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
